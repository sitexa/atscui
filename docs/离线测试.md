# 脱离仿真环境的模型测试工具(2025-7-14)

本文档介绍如何在不启动SUMO仿真环境的情况下测试训练好的强化学习模型。

## 背景

在原有的测试方式中（如 `test_run_model.py`），模型测试需要启动完整的SUMO仿真环境，这样做有以下限制：

1. **依赖仿真环境**：需要SUMO软件和网络文件
2. **测试速度慢**：仿真环境启动和运行需要时间
3. **难以控制输入**：观测数据由仿真环境生成，难以测试特定场景
4. **调试困难**：无法精确控制输入来分析模型行为

## 解决方案

我们提供了两个脱离仿真环境的测试工具：

### 1. 简单快速测试 - `test_model_simple.py`

**用途**：快速验证模型是否能正常加载和预测

**特点**：
- 轻量级，运行速度快
- 基本功能验证
- 一致性测试
- 预定义场景测试

**使用方法**：
```bash
cd /Users/xnpeng/sumoptis/atscui
python atscui/test_model_simple.py
```

### 2. 详细离线测试 - `test_model_offline.py`

**用途**：全面测试模型在各种场景下的表现

**特点**：
- 多种测试场景
- 边界情况测试
- 现实交通场景模拟
- 详细的测试报告

**使用方法**：
```bash
cd /Users/xnpeng/sumoptis/atscui
python atscui/test_model_offline.py
```

## 测试内容

### 基本功能测试
1. **模型加载测试**：验证模型文件是否能正确加载
2. **预测功能测试**：验证模型能否正常进行预测
3. **一致性测试**：相同输入是否产生相同输出
4. **维度兼容性测试**：观测空间和动作空间是否匹配

### 场景测试
1. **低流量场景**：模拟交通流量较少的情况
2. **高流量场景**：模拟交通拥堵的情况
3. **不均衡流量**：模拟各方向流量不均的情况
4. **紧急情况**：模拟需要快速响应的场景

### 边界测试
1. **全零输入**：测试极端情况下的模型行为
2. **全一输入**：测试饱和状态下的模型行为
3. **随机输入**：测试模型的鲁棒性

## 观测数据格式

模型期望的观测向量（45维）包含：

```
观测向量 = [相位编码(4维) + 最小绿灯标志(1维) + 车道密度(20维) + 车道队列(20维) + CCI(1维) + 控制模式(1维)]
```

- **相位编码**：独热编码，表示当前激活的绿灯相位
- **最小绿灯标志**：0或1，表示是否满足最小绿灯时间
- **车道密度**：0-1之间的值，表示各车道的车辆密度
- **车道队列**：0-1之间的值，表示各车道的排队长度
- **CCI**：0-1之间的值，表示当前的拥堵指数
- **控制模式**：0或1，表示控制模式（0=顺序，1=灵活）

## 支持的算法

目前支持以下强化学习算法：
- **DQN**：Deep Q-Network
- **PPO**：Proximal Policy Optimization
- **A2C**：Advantage Actor-Critic
- **SAC**：Soft Actor-Critic

## 示例输出

### 快速测试输出示例
```
快速测试 DQN 模型
模型路径: /Users/xnpeng/sumoptis/atscui/models/zfdx-model-DQN.zip
✅ 模型加载成功
   观测空间: Box(0.0, 1.0, (45,), float32)
   动作空间: Discrete(4)
✅ 模型预测成功
   输入观测维度: (1, 45)
   输出动作: [2]
   动作类型: <class 'numpy.ndarray'>
✅ 一致性测试: 通过
```

### 场景测试输出示例
```
场景测试结果:
  低流量场景: 动作 = [0]
  高流量场景: 动作 = [1]
  混合场景: 动作 = [2]
✅ 场景测试完成
```

## 优势

与原有的仿真环境测试相比，脱离仿真环境的测试具有以下优势：

1. **独立性**：不依赖SUMO仿真环境，可以在任何环境中运行
2. **速度快**：无需启动仿真，测试速度大幅提升
3. **可控性**：可以精确控制输入数据，测试特定场景
4. **可重复性**：测试结果完全可重复
5. **调试友好**：便于分析模型在特定输入下的行为
6. **批量测试**：可以快速测试多个模型

## 注意事项

1. **观测维度**：确保测试数据的维度与模型训练时的观测空间一致
2. **数据范围**：观测数据应在[0,1]范围内，与训练时的归一化保持一致
3. **模型路径**：确保模型文件路径正确且文件存在
4. **算法匹配**：使用正确的算法类型加载对应的模型文件

## 扩展使用

### 自定义测试场景

可以在代码中添加自定义的测试场景：

```python
# 自定义场景
custom_scenario = np.array([
    1, 0, 0, 0,  # 相位0激活
    1,           # 满足最小绿灯时间
    # ... 自定义的密度和队列数据
], dtype=np.float32)

action, _ = model.predict(custom_scenario.reshape(1, -1))
print(f"自定义场景预测动作: {action}")
```

### 批量测试多个模型

可以修改脚本来测试多个模型文件：

```python
models = [
    ("model1.zip", "DQN"),
    ("model2.zip", "SAC"),
    # 添加更多模型
]

for model_path, algo in models:
    test_model_offline(model_path, algo)
```

这样就可以实现真正脱离仿真环境的模型测试，大大提高了测试效率和灵活性。


# 测试结果分析： 当前DQN模型在决策上可能存在的单一性问题

##  问题原因分析


  强化学习模型（如DQN）的行为完全由其“学习目标”驱动，即最大化累积奖励。如果模型总是选择同一个动作，最根本的原因是，在它的“认知”里，这个动作在绝大多数情况下都能带来最高的（或最不差的）预期回报。

  具体到我们的交通信号控制场景，可能的原因有以下几点：


   1. 奖励函数设计不合理（最可能的原因）:
       * 过于笼统: 当前的奖励函数可能过于简单，比如只考虑了路网的总等待时间或总延误。在这种情况下，如果“南北左转”
         这个相位能稍微缓解一个主要方向的拥堵，即使其他方向的拥堵在累积，它也可能在每个时间步都获得一个正向的（或负向最小的）奖励，从而被模型固化为最优策略。
       * 缺乏对“压力”的感知:
         一个好的奖励函数应该能让智能体感知到路口各个方向的“交通压力”变化。如果一个方向的车辆排队很长，等待时间很久，那么服务这个方向的“紧迫性”就应该更高。当前的奖励函数可能没有很好地体现这种紧迫性差异。


   2. 状态（Observation）表示不充分:
       * 虽然我们已经有了密度、队列等信息，但可能缺少一些关键特征来让模型区分不同的交通状态。例如，模型可能无法有效地区分“两个方向都比较拥堵”和“一个方向极度拥堵而另一个方向畅通”这两种情况，导致它在这两种情况下都采取了相同的“平均最优”策略。


   3. 训练过程的问题:
       * 探索不足: 在DQN训练初期，如果探索（Exploration）不够充分，模型可能过早地收敛到了一个“局部最优解”（比如总是选择南北左转），而没有机会发现其他更优的策略组合。
       * 训练场景单一: 如果训练时使用的车流文件（.rou.xml）过于单一或有偏向性（例如，南北方向的车流量在大多数训练场景中都恰好是主要矛盾），模型就会学到只针对这种特定情况的策略，缺乏泛化能力。

##  解决方案

  最直接、最有效的解决方案是从奖励函数入手。我们需要设计一个新的奖励函数，让它更能体现交通控制的“智慧”，即优先服务于最紧急、最拥堵的方向。


  核心思想：引入“路口压力（Intersection Pressure）”的概念。

  “压力”可以被定义为冲突相位之间车辆排队长度的差异。一个相位上的压力越大，意味着释放这个相位的需求越迫切。我们的奖励应该鼓励那些能够最大程度缓解路口整体压力的行为。

##  具体方案：


   1. 定义“车道压力”: 对于每一条进入路口的车道，其压力可以简单地定义为该车道上正在排队等待的车辆数。
   2. 定义“相位压力”: 一个信号相位（比如“南北左转”）的压力，是所有与该相位绿灯冲突、且当前是红灯的那些车道的“车道压力”之和。简单来说，就是当前相位绿灯时，红灯方向有多少车在等着。
   3. 定义“路口总压力”: 整个路口的总压力就是所有相位压力中的最大值。
   4. 设计新的奖励函数: 奖励不再是某个单一指标，而是路口总压力的变化量。即 reward = old_pressure - new_pressure。
       * 如果一个动作（切换相位）导致路口总压力下降（new_pressure < old_pressure），那么奖励就是正的，模型会受到鼓励。
       * 如果一个动作导致压力上升，奖励就是负的，模型会受到惩罚。


  通过这种方式，智能体会被迫去关注那些压力最大的方向，并采取行动来缓解它，而不是墨守成规地选择同一个相位。

##  行动计划


  要实现这个方案，我需要修改环境定义文件，很可能在 sumo_core/envs/sumo_env.py 或
  sumo_core/envs/traffic_signal.py 中。

  我的计划如下：


   1. 分析代码：首先，我会阅读 sumo_core/envs/sumo_env.py 和
      sumo_core/envs/traffic_signal.py，找到当前计算奖励（reward）和获取车道车辆信息的核心逻辑。
   2. 提出修改：然后，我会基于上述“压力”概念，提出具体的代码修改方案，在 SumoEnv 类中实现新的 _compute_reward方法。
   3. 征求同意：在获得您的同意后，我将执行代码修改。


  这个方案旨在从根本上优化模型的决策逻辑，使其更加智能和自适应。

