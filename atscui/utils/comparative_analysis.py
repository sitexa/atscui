#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¹æ¯”åˆ†æå·¥å…· - é›†æˆåˆ°ATSCUIç³»ç»Ÿ
åŸºäºvisualization_tab.pyçš„è®¾è®¡æ¨¡å¼ï¼Œæä¾›PPOä¸å›ºå®šå‘¨æœŸçš„å¯¹æ¯”åˆ†æåŠŸèƒ½
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Tuple, Optional, List, Dict
import traceback
import glob
import re

# å¤„ç†æ¨¡å—å¯¼å…¥é—®é¢˜ - å½“ä½œä¸ºç‹¬ç«‹è„šæœ¬è¿è¡Œæ—¶
try:
    from atscui.logging_manager import get_logger
    from atscui.utils.file_utils import FileManager
    from atscui.exceptions import ValidationError, FileOperationError
except ImportError:
    # å½“ä½œä¸ºç‹¬ç«‹è„šæœ¬è¿è¡Œæ—¶ï¼Œæ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    current_dir = Path(__file__).resolve().parent
    project_root = current_dir.parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    try:
        from atscui.logging_manager import get_logger
        from atscui.utils.file_utils import FileManager
        from atscui.exceptions import ValidationError, FileOperationError
    except ImportError:
        # å¦‚æœä»ç„¶æ— æ³•å¯¼å…¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
        import logging
        
        def get_logger(name):
            logger = logging.getLogger(name)
            if not logger.handlers:
                handler = logging.StreamHandler()
                formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
                handler.setFormatter(formatter)
                logger.addHandler(handler)
                logger.setLevel(logging.INFO)
            return logger
        
        class FileManager:
            @staticmethod
            def file_exists(path):
                return os.path.exists(path)
        
        class ValidationError(Exception):
            pass
        
        class FileOperationError(Exception):
            pass

# è®¾ç½®ä¸­æ–‡å­—ä½“ - å…¼å®¹å¤šæ“ä½œç³»ç»Ÿ
def setup_chinese_fonts():
    """è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œæ™ºèƒ½é€‚é…ä¸åŒæ“ä½œç³»ç»Ÿ"""
    import matplotlib.font_manager as fm
    import platform
    
    # è·å–æ“ä½œç³»ç»Ÿä¿¡æ¯
    system = platform.system().lower()
    
    # æ ¹æ®æ“ä½œç³»ç»Ÿå®šä¹‰å­—ä½“ä¼˜å…ˆçº§
    if system == 'darwin':  # macOS
        font_list = [
            'Arial Unicode MS',  # macOSæœ€ä½³ä¸­æ–‡æ”¯æŒ
            'PingFang SC',       # macOSç³»ç»Ÿä¸­æ–‡å­—ä½“
            'Hiragino Sans GB',  # macOSä¸­æ–‡å­—ä½“
            'STHeiti',           # macOSä¸­æ–‡å­—ä½“
            'Arial',             # è‹±æ–‡å­—ä½“
            'Helvetica',         # macOSè‹±æ–‡å­—ä½“
            'sans-serif'         # ç³»ç»Ÿé»˜è®¤
        ]
    elif system == 'linux':  # Ubuntu/Linux
        font_list = [
            'Noto Sans CJK SC',      # Linuxæœ€ä½³ä¸­æ–‡æ”¯æŒ
            'WenQuanYi Micro Hei',   # Linuxä¸­æ–‡å­—ä½“
            'WenQuanYi Zen Hei',     # Linuxä¸­æ–‡å­—ä½“
            'Droid Sans Fallback',   # Android/Linuxå­—ä½“
            'Liberation Sans',       # Linuxè‹±æ–‡å­—ä½“
            'DejaVu Sans',          # Linuxè‹±æ–‡å­—ä½“
            'Ubuntu',               # Ubuntuå­—ä½“
            'Noto Sans',            # é€šç”¨å­—ä½“
            'Arial',                # é€šç”¨è‹±æ–‡å­—ä½“
            'sans-serif'            # ç³»ç»Ÿé»˜è®¤
        ]
    else:  # Windowsæˆ–å…¶ä»–ç³»ç»Ÿ
        font_list = [
            'Microsoft YaHei',   # Windowsä¸­æ–‡å­—ä½“
            'SimHei',           # Windowsä¸­æ–‡å­—ä½“
            'Arial Unicode MS', # Windowsä¸­æ–‡æ”¯æŒ
            'Arial',            # è‹±æ–‡å­—ä½“
            'Helvetica',        # è‹±æ–‡å­—ä½“
            'sans-serif'        # ç³»ç»Ÿé»˜è®¤
        ]
    
    # æ£€æŸ¥å¯ç”¨å­—ä½“
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    # æ‰¾åˆ°å¯ç”¨çš„å­—ä½“
    selected_fonts = []
    for font in font_list:
        if font in available_fonts:
            selected_fonts.append(font)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å­—ä½“ï¼Œæ·»åŠ é€šç”¨å¤‡é€‰
    if not selected_fonts:
        selected_fonts = ['DejaVu Sans', 'Arial', 'sans-serif']
        print(f"è­¦å‘Š: æœªæ‰¾åˆ°ç³»ç»Ÿæ¨èå­—ä½“ï¼Œä½¿ç”¨é€šç”¨å­—ä½“: {selected_fonts[0]}")
        if system == 'linux':
            print("å»ºè®®å®‰è£…ä¸­æ–‡å­—ä½“åŒ…: sudo apt-get install fonts-noto-cjk")
    else:
        print(f"æ£€æµ‹åˆ°æ“ä½œç³»ç»Ÿ: {system.upper()}")
        print(f"ä½¿ç”¨å­—ä½“: {selected_fonts[0]} (å…±æ‰¾åˆ° {len(selected_fonts)} ä¸ªå¯ç”¨å­—ä½“)")
    
    # è®¾ç½®matplotlibå­—ä½“é…ç½®
    plt.rcParams['font.sans-serif'] = selected_fonts
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
    plt.rcParams['font.family'] = ['sans-serif']  # è®¾ç½®å­—ä½“æ—
    
    # é¢å¤–çš„æ¸²æŸ“è®¾ç½®ï¼Œæé«˜å…¼å®¹æ€§
    plt.rcParams['axes.formatter.use_mathtext'] = True
    plt.rcParams['mathtext.fontset'] = 'stix'  # æ•°å­¦æ–‡æœ¬å­—ä½“
    
    return selected_fonts[0] if selected_fonts else 'sans-serif'

# åˆå§‹åŒ–ä¸­æ–‡å­—ä½“è®¾ç½®
setup_chinese_fonts()

class ComparativeAnalyzer:
    """å¯¹æ¯”åˆ†æå™¨ - ç”¨äºPPOä¸å›ºå®šå‘¨æœŸçš„æ€§èƒ½å¯¹æ¯”"""
    
    def __init__(self):
        self.logger = get_logger('comparative_analyzer')
        self.file_manager = FileManager()
        
    def analyze_training_comparison(self, sample_file_path: str) -> Tuple[Optional[str], str]:
        """
        åŸºäºæ ·æœ¬æ–‡ä»¶è·¯å¾„åˆ†æåŒè·¯å£å¤šç®—æ³•è®­ç»ƒæ—¥å¿—çš„å¯¹æ¯”
        
        Args:
            sample_file_path: æ ·æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚zfdx-PPO_conn0_ep1.csvï¼‰
            
        Returns:
            Tuple[Optional[str], str]: (åˆ†æç»“æœå›¾ç‰‡è·¯å¾„, è¾“å‡ºä¿¡æ¯)
        """
        try:
            # 1. éªŒè¯è¾“å…¥æ–‡ä»¶
            if not sample_file_path:
                return None, "âŒ è¯·æä¾›æ ·æœ¬æ–‡ä»¶è·¯å¾„"
                
            if not self.file_manager.file_exists(sample_file_path):
                return None, f"âŒ æ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {sample_file_path}"
            
            # 2. è§£æè·¯å£åç§°å’Œç›®å½•
            base_dir = os.path.dirname(sample_file_path)
            filename = os.path.basename(sample_file_path)
            
            # æå–è·¯å£åç§°ï¼ˆå¦‚zfdxï¼‰
            match = re.match(r'([^-]+)-', filename)
            if not match:
                return None, f"âŒ æ— æ³•ä»æ–‡ä»¶åæå–è·¯å£åç§°: {filename}"
            
            intersection_name = match.group(1)
            self.logger.info(f"æ£€æµ‹åˆ°è·¯å£åç§°: {intersection_name}")
            
            # 3. æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³æ–‡ä»¶
            file_groups = self._find_algorithm_files(base_dir, intersection_name)
            
            if not any(file_groups.values()):
                return None, f"âŒ æœªæ‰¾åˆ°è·¯å£ {intersection_name} çš„ä»»ä½•ç®—æ³•æ—¥å¿—æ–‡ä»¶"
            
            # 4. åŠ è½½å’Œç»Ÿè®¡æ•°æ®
            self.logger.info("å¼€å§‹åŠ è½½å’Œç»Ÿè®¡è®­ç»ƒæ•°æ®...")
            algorithm_stats = self._load_and_aggregate_data(file_groups)
            
            # 5. æ‰§è¡Œå¯¹æ¯”åˆ†æ
            analysis_results = self._perform_multi_algorithm_analysis(algorithm_stats, intersection_name)
            
            # 6. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
            plot_path = os.path.join(base_dir, f"{intersection_name}_multi_algorithm_analysis.png")
            self._create_multi_algorithm_plots(algorithm_stats, analysis_results, plot_path)
            
            # 7. ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report_path = os.path.join(base_dir, f"{intersection_name}_multi_algorithm_report.md")
            self._generate_multi_algorithm_report(algorithm_stats, analysis_results, report_path)
            
            success_msg = f"âœ… å¤šç®—æ³•å¯¹æ¯”åˆ†æå®Œæˆ\n" \
                         f"ğŸš¦ è·¯å£: {intersection_name}\n" \
                         f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨: {plot_path}\n" \
                         f"ğŸ“„ åˆ†ææŠ¥å‘Š: {report_path}\n" \
                         f"ğŸ” åˆ†æç®—æ³•: {', '.join(algorithm_stats.keys())}"
            
            self.logger.info("å¤šç®—æ³•å¯¹æ¯”åˆ†ææˆåŠŸå®Œæˆ")
            return plot_path, success_msg
            
        except ValidationError as e:
            error_msg = f"âŒ è¾“å…¥éªŒè¯å¤±è´¥: {e}"
            self.logger.warning(error_msg)
            return None, error_msg
            
        except FileOperationError as e:
            error_msg = f"âŒ æ–‡ä»¶æ“ä½œå¤±è´¥: {e}"
            self.logger.error(error_msg)
            return None, error_msg
            
        except Exception as e:
            error_msg = f"âŒ å¯¹æ¯”åˆ†ææ—¶å‘ç”Ÿé”™è¯¯: {e}"
            self.logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return None, error_msg
    
    def _find_algorithm_files(self, base_dir: str, intersection_name: str) -> Dict[str, List[str]]:
        """æŸ¥æ‰¾æŒ‡å®šè·¯å£çš„æ‰€æœ‰ç®—æ³•æ—¥å¿—æ–‡ä»¶"""
        # å®šä¹‰æ”¯æŒçš„ç®—æ³•ç±»å‹
        rl_algorithms = ['PPO', 'DQN', 'A2C', 'SAC']
        fixtime_algorithms = ['FIXTIME-curriculum', 'FIXTIME-static']
        
        file_groups = {}
        
        # åˆå§‹åŒ–æ–‡ä»¶ç»„
        for alg in rl_algorithms + fixtime_algorithms:
            file_groups[alg] = []
        
        # å¼ºåŒ–å­¦ä¹ ç®—æ³•æ–‡ä»¶æ¨¡å¼: è·¯å£-ç®—æ³•_conn0_ep*.csv
        for algorithm in rl_algorithms:
            pattern = os.path.join(base_dir, f"{intersection_name}-{algorithm}_conn0_ep*.csv")
            file_groups[algorithm] = glob.glob(pattern)
        
        # fixtimeç®—æ³•æ–‡ä»¶æ¨¡å¼: è·¯å£-fixtime-ç±»å‹-*.csv
        for algorithm in fixtime_algorithms:
            pattern = os.path.join(base_dir, f"{intersection_name}-{algorithm}-*.csv")
            file_groups[algorithm] = glob.glob(pattern)
        
        # è®°å½•æ‰¾åˆ°çš„æ–‡ä»¶æ•°é‡
        found_info = []
        for alg, files in file_groups.items():
            if files:
                found_info.append(f"{alg}({len(files)})")
        
        self.logger.info(f"æ‰¾åˆ°æ–‡ä»¶: {', '.join(found_info) if found_info else 'æ— '}")
        
        return file_groups
    
    def _load_and_aggregate_data(self, file_groups: Dict[str, List[str]]) -> Dict[str, pd.DataFrame]:
        """åŠ è½½å¹¶èšåˆå„ç®—æ³•çš„æ•°æ®"""
        algorithm_stats = {}
        
        for algorithm, files in file_groups.items():
            if not files:
                continue
                
            all_data = []
            for file_path in files:
                try:
                    data = pd.read_csv(file_path)
                    # æ·»åŠ æ–‡ä»¶æ ‡è¯†
                    data['source_file'] = os.path.basename(file_path)
                    all_data.append(data)
                    self.logger.info(f"åŠ è½½ {algorithm} æ–‡ä»¶: {os.path.basename(file_path)} ({len(data)}æ¡è®°å½•)")
                except Exception as e:
                    self.logger.warning(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    continue
            
            if all_data:
                # åˆå¹¶æ‰€æœ‰æ•°æ®
                combined_data = pd.concat(all_data, ignore_index=True)
                
                # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                stats = self._calculate_algorithm_statistics(combined_data, algorithm)
                algorithm_stats[algorithm] = stats
                
                self.logger.info(f"{algorithm} æ€»è®¡: {len(combined_data)}æ¡è®°å½•, {len(all_data)}ä¸ªæ–‡ä»¶")
        
        return algorithm_stats
    
    def _calculate_algorithm_statistics(self, data: pd.DataFrame, algorithm: str) -> Dict:
        """è®¡ç®—å•ä¸ªç®—æ³•çš„ç»Ÿè®¡æŒ‡æ ‡"""
        stats = {
            'raw_data': data,
            'total_records': len(data),
            'file_count': data['source_file'].nunique(),
            'metrics': {}
        }
        
        # å®šä¹‰å…³é”®æŒ‡æ ‡
        key_metrics = [
            'system_mean_waiting_time',
            'system_mean_speed', 
            'system_total_throughput',
            'system_mean_travel_time',
            'system_total_fuel_consumption',
            'system_total_co2_emission'
        ]
        
        # å¯¹äºå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä½¿ç”¨åæœŸé˜¶æ®µçš„å¹³å‡å€¼
        rl_algorithms = ['PPO', 'DQN', 'A2C', 'SAC']
        if algorithm in rl_algorithms and len(data) > 100:
            # ä½¿ç”¨æœ€å1/4çš„æ•°æ®ä½œä¸ºç¨³å®šæœŸæ€§èƒ½
            stable_data = data.iloc[-len(data)//4:]
        else:
            stable_data = data
        
        # è®¡ç®—å„æŒ‡æ ‡çš„ç»Ÿè®¡å€¼
        for metric in key_metrics:
            if metric in stable_data.columns:
                if metric == 'system_total_throughput':
                    # å¯¹äºæ€»é€šè¡Œé‡ï¼Œè®¡ç®—å¹³å‡é€šè¡Œé‡ï¼ˆå‡è®¾æ¯æ¡è®°å½•ä»£è¡¨ä¸€ä¸ªæ—¶é—´æ­¥é•¿ï¼‰
                    # å‡è®¾æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸º1ç§’ï¼Œè½¬æ¢ä¸ºæ¯å°æ—¶é€šè¡Œé‡
                    # å¦‚æœæ•°æ®ä¸­æœ‰æ—¶é—´ä¿¡æ¯ï¼Œå¯ä»¥æ›´ç²¾ç¡®è®¡ç®—
                    avg_throughput_per_step = stable_data[metric].mean()
                    # å‡è®¾æ¯æ­¥ä¸º1ç§’ï¼Œè½¬æ¢ä¸ºæ¯å°æ—¶ï¼ˆ3600ç§’ï¼‰
                    hourly_throughput = avg_throughput_per_step * 3600 / len(stable_data) if len(stable_data) > 0 else 0
                    
                    stats['metrics'][metric] = {
                        'mean': hourly_throughput,  # ä½¿ç”¨æ¯å°æ—¶å¹³å‡é€šè¡Œé‡
                        'std': stable_data[metric].std() * 3600 / len(stable_data) if len(stable_data) > 0 else 0,
                        'min': stable_data[metric].min(),
                        'max': stable_data[metric].max(),
                        'median': stable_data[metric].median()
                    }
                    # åŒæ—¶ä¿å­˜åŸå§‹æ€»é€šè¡Œé‡ç”¨äºå‚è€ƒ
                    stats['metrics']['system_total_throughput_raw'] = {
                        'mean': stable_data[metric].mean(),
                        'std': stable_data[metric].std(),
                        'min': stable_data[metric].min(),
                        'max': stable_data[metric].max(),
                        'median': stable_data[metric].median()
                    }
                else:
                    stats['metrics'][metric] = {
                        'mean': stable_data[metric].mean(),
                        'std': stable_data[metric].std(),
                        'min': stable_data[metric].min(),
                        'max': stable_data[metric].max(),
                        'median': stable_data[metric].median()
                    }
        
        # å…¼å®¹æ—§æ ¼å¼çš„åˆ—åæ˜ å°„
        legacy_mapping = {
            'avg_waiting_time': 'system_mean_waiting_time',
            'avg_speed': 'system_mean_speed',
            'total_throughput': 'system_total_throughput',
            'avg_travel_time': 'system_mean_travel_time',
            'total_fuel_consumption': 'system_total_fuel_consumption',
            'total_co2_emission': 'system_total_co2_emission'
        }
        
        for legacy_col, standard_col in legacy_mapping.items():
            if legacy_col in stable_data.columns and standard_col not in stats['metrics']:
                stats['metrics'][standard_col] = {
                    'mean': stable_data[legacy_col].mean(),
                    'std': stable_data[legacy_col].std(),
                    'min': stable_data[legacy_col].min(),
                    'max': stable_data[legacy_col].max(),
                    'median': stable_data[legacy_col].median()
                }
        
        return stats
    
    def _perform_multi_algorithm_analysis(self, algorithm_stats: Dict, intersection_name: str) -> Dict:
        """æ‰§è¡Œå¤šç®—æ³•å¯¹æ¯”åˆ†æ"""
        analysis_results = {
            'intersection_name': intersection_name,
            'algorithms': list(algorithm_stats.keys()),
            'comparison_matrix': {},
            'best_performers': {},
            'improvement_analysis': {}
        }
        
        # å®šä¹‰æŒ‡æ ‡å’Œä¼˜åŒ–æ–¹å‘ï¼ˆTrueè¡¨ç¤ºè¶Šå¤§è¶Šå¥½ï¼ŒFalseè¡¨ç¤ºè¶Šå°è¶Šå¥½ï¼‰
        metrics_direction = {
            'system_mean_waiting_time': False,  # ç­‰å¾…æ—¶é—´è¶Šå°è¶Šå¥½
            'system_mean_speed': True,          # é€Ÿåº¦è¶Šå¤§è¶Šå¥½
            'system_total_throughput': True,    # å¹³å‡é€šè¡Œé‡è¶Šå¤§è¶Šå¥½
            'system_mean_travel_time': False,   # è¡Œç¨‹æ—¶é—´è¶Šå°è¶Šå¥½
            'system_total_fuel_consumption': False,  # ç‡ƒæ²¹æ¶ˆè€—è¶Šå°è¶Šå¥½
            'system_total_co2_emission': False       # CO2æ’æ”¾è¶Šå°è¶Šå¥½
        }
        
        # æ„å»ºå¯¹æ¯”çŸ©é˜µ
        for metric, is_higher_better in metrics_direction.items():
            metric_comparison = {}
            metric_values = {}
            
            # æ”¶é›†å„ç®—æ³•åœ¨è¯¥æŒ‡æ ‡ä¸Šçš„è¡¨ç°
            for algorithm, stats in algorithm_stats.items():
                if metric in stats['metrics']:
                    metric_values[algorithm] = stats['metrics'][metric]['mean']
            
            if len(metric_values) > 1:
                # æ‰¾å‡ºæœ€ä½³è¡¨ç°
                if is_higher_better:
                    best_algorithm = max(metric_values, key=metric_values.get)
                    best_value = max(metric_values.values())
                else:
                    best_algorithm = min(metric_values, key=metric_values.get)
                    best_value = min(metric_values.values())
                
                analysis_results['best_performers'][metric] = {
                    'algorithm': best_algorithm,
                    'value': best_value
                }
                
                # è®¡ç®—ç›¸å¯¹æ”¹å–„ç‡
                for algorithm, value in metric_values.items():
                    if algorithm != best_algorithm:
                        if is_higher_better:
                            improvement = (best_value - value) / value * 100
                        else:
                            improvement = (value - best_value) / value * 100
                        
                        metric_comparison[f"{best_algorithm}_vs_{algorithm}"] = {
                            'improvement_rate': improvement,
                            'best_value': best_value,
                            'compared_value': value
                        }
            
            analysis_results['comparison_matrix'][metric] = {
                'values': metric_values,
                'comparisons': metric_comparison
            }
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        self._calculate_overall_scores(analysis_results, algorithm_stats)
        
        return analysis_results
    
    def _calculate_overall_scores(self, analysis_results: Dict, algorithm_stats: Dict):
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        algorithms = analysis_results['algorithms']
        overall_scores = {alg: 0 for alg in algorithms}
        
        # ä¸ºæ¯ä¸ªæŒ‡æ ‡åˆ†é…æƒé‡
        metric_weights = {
            'system_mean_waiting_time': 0.25,    # å¹³å‡ç­‰å¾…æ—¶é—´
            'system_mean_speed': 0.20,           # å¹³å‡é€Ÿåº¦
            'system_total_throughput': 0.20,     # å¹³å‡é€šè¡Œé‡ï¼ˆæ¯å°æ—¶ï¼‰
            'system_mean_travel_time': 0.15,     # å¹³å‡è¡Œç¨‹æ—¶é—´
            'system_total_fuel_consumption': 0.10, # æ€»ç‡ƒæ²¹æ¶ˆè€—
            'system_total_co2_emission': 0.10    # æ€»CO2æ’æ”¾
        }
        
        for metric, weight in metric_weights.items():
            if metric in analysis_results['best_performers']:
                best_alg = analysis_results['best_performers'][metric]['algorithm']
                overall_scores[best_alg] += weight
        
        analysis_results['overall_scores'] = overall_scores
        analysis_results['best_overall'] = max(overall_scores, key=overall_scores.get)
    
    def _perform_comparative_analysis(self, ppo_data: pd.DataFrame, fixtime_data: pd.DataFrame) -> dict:
        """æ‰§è¡Œå¯¹æ¯”åˆ†æè®¡ç®—"""
        # è·å–PPOæœ€ç»ˆæ€§èƒ½ï¼ˆåæœŸé˜¶æ®µå¹³å‡å€¼ï¼‰
        ppo_final = ppo_data.iloc[-len(ppo_data)//4:]  # æœ€å1/4æ•°æ®
        fixtime_result = fixtime_data.iloc[0]  # å›ºå®šå‘¨æœŸç»“æœ
        
        # å®šä¹‰å¯¹æ¯”æŒ‡æ ‡æ˜ å°„
        comparison_metrics = {
            'å¹³å‡ç­‰å¾…æ—¶é—´': ('avg_waiting_time', 'system_mean_waiting_time'),
            'å¹³å‡é€Ÿåº¦': ('avg_speed', 'system_mean_speed'),
            'æ€»é€šè¡Œé‡': ('total_throughput', 'system_total_throughput'),
            'å¹³å‡è¡Œç¨‹æ—¶é—´': ('avg_travel_time', 'system_mean_travel_time'),
            'æ€»ç‡ƒæ²¹æ¶ˆè€—': ('total_fuel_consumption', 'system_total_fuel_consumption'),
            'æ€»CO2æ’æ”¾': ('total_co2_emission', 'system_total_co2_emission')
        }
        
        improvements = {}
        comparison_data = {}
        
        for metric_name, (fixtime_col, ppo_col) in comparison_metrics.items():
            if fixtime_col in fixtime_data.columns and ppo_col in ppo_final.columns:
                fixtime_val = fixtime_result[fixtime_col]
                ppo_val = ppo_final[ppo_col].mean()
                
                # è®¡ç®—æ”¹å–„ç‡
                if metric_name in ['å¹³å‡ç­‰å¾…æ—¶é—´', 'å¹³å‡è¡Œç¨‹æ—¶é—´', 'æ€»ç‡ƒæ²¹æ¶ˆè€—', 'æ€»CO2æ’æ”¾']:
                    improvement = (fixtime_val - ppo_val) / fixtime_val * 100
                else:
                    improvement = (ppo_val - fixtime_val) / fixtime_val * 100
                
                improvements[metric_name] = improvement
                comparison_data[metric_name] = {
                    'fixtime': fixtime_val,
                    'ppo': ppo_val,
                    'improvement': improvement
                }
        
        # åˆ†æPPOè®­ç»ƒè¿‡ç¨‹
        total_steps = len(ppo_data)
        training_phases = {
            'åˆæœŸ': ppo_data.iloc[:total_steps//3],
            'ä¸­æœŸ': ppo_data.iloc[total_steps//3:2*total_steps//3],
            'åæœŸ': ppo_data.iloc[2*total_steps//3:]
        }
        
        phase_analysis = {}
        for phase_name, phase_data in training_phases.items():
            phase_analysis[phase_name] = {
                'avg_waiting': phase_data['system_mean_waiting_time'].mean(),
                'avg_speed': phase_data['system_mean_speed'].mean(),
                'avg_throughput': phase_data['system_total_throughput'].mean()
            }
        
        return {
            'improvements': improvements,
            'comparison_data': comparison_data,
            'phase_analysis': phase_analysis,
            'avg_improvement': np.mean(list(improvements.values())),
            'ppo_stats': {
                'total_records': len(ppo_data),
                'total_steps': ppo_data['step'].max(),
                'step_interval': ppo_data['step'].iloc[1] - ppo_data['step'].iloc[0]
            },
            'fixtime_stats': {
                'total_records': len(fixtime_data),
                'total_steps': fixtime_data['total_steps'].iloc[0] if 'total_steps' in fixtime_data.columns else 'N/A'
            }
        }
    
    def _create_multi_algorithm_plots(self, algorithm_stats: Dict, analysis_results: Dict, output_path: str):
        """åˆ›å»ºå¤šç®—æ³•å¯¹æ¯”åˆ†æå›¾è¡¨"""
        algorithms = list(algorithm_stats.keys())
        n_algorithms = len(algorithms)
        
        if n_algorithms == 0:
            self.logger.warning("æ²¡æœ‰ç®—æ³•æ•°æ®å¯ä¾›ç»˜å›¾")
            return
        
        # è®¾ç½®å›¾è¡¨å¸ƒå±€
        fig = plt.figure(figsize=(20, 16))
        gs = fig.add_gridspec(3, 3, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1])
        
        # å®šä¹‰é¢œè‰²æ˜ å°„
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
        algorithm_colors = {alg: colors[i % len(colors)] for i, alg in enumerate(algorithms)}
        
        # 1. å…³é”®æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾ (ä¸Šæ’)
        key_metrics = [
            ('system_mean_waiting_time', 'å¹³å‡ç­‰å¾…æ—¶é—´(ç§’)'),
            ('system_mean_speed', 'å¹³å‡é€Ÿåº¦(m/s)'),
            ('system_total_throughput', 'å¹³å‡é€šè¡Œé‡(è¾†/å°æ—¶)')
        ]
        
        for i, (metric, title) in enumerate(key_metrics):
            ax = fig.add_subplot(gs[0, i])
            
            values = []
            labels = []
            bar_colors = []
            
            for algorithm in algorithms:
                if metric in algorithm_stats[algorithm]['metrics']:
                    values.append(algorithm_stats[algorithm]['metrics'][metric]['mean'])
                    labels.append(algorithm)
                    bar_colors.append(algorithm_colors[algorithm])
            
            if values:
                bars = ax.bar(labels, values, color=bar_colors, alpha=0.7)
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{height:.1f}', ha='center', va='bottom', fontweight='bold')
                
                # æ ‡è®°æœ€ä½³è¡¨ç°
                if metric in analysis_results['best_performers']:
                    best_alg = analysis_results['best_performers'][metric]['algorithm']
                    best_idx = labels.index(best_alg) if best_alg in labels else -1
                    if best_idx >= 0:
                        bars[best_idx].set_edgecolor('gold')
                        bars[best_idx].set_linewidth(3)
                
                ax.set_title(f'{title}å¯¹æ¯”', fontweight='bold', fontsize=12)
                ax.set_ylabel(title)
                ax.tick_params(axis='x', rotation=45)
        
        # 2. ç»¼åˆè¯„åˆ†é›·è¾¾å›¾ (ä¸­æ’å·¦)
        if len(algorithms) > 1:
            ax = fig.add_subplot(gs[1, 0])
            overall_scores = analysis_results['overall_scores']
            
            algorithms_list = list(overall_scores.keys())
            scores = list(overall_scores.values())
            
            bars = ax.bar(algorithms_list, scores, 
                         color=[algorithm_colors[alg] for alg in algorithms_list], alpha=0.7)
            
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.2f}', ha='center', va='bottom', fontweight='bold')
            
            ax.set_title('ç»¼åˆè¯„åˆ†å¯¹æ¯”', fontweight='bold', fontsize=12)
            ax.set_ylabel('ç»¼åˆè¯„åˆ†')
            ax.tick_params(axis='x', rotation=45)
        
        # 3. å„æŒ‡æ ‡æœ€ä½³è¡¨ç°è€… (ä¸­æ’ä¸­)
        ax = fig.add_subplot(gs[1, 1])
        ax.axis('off')
        
        best_performers_text = "å„æŒ‡æ ‡æœ€ä½³è¡¨ç°:\n\n"
        for metric, performer in analysis_results['best_performers'].items():
            metric_name = {
                'system_mean_waiting_time': 'ç­‰å¾…æ—¶é—´',
                'system_mean_speed': 'å¹³å‡é€Ÿåº¦',
                'system_total_throughput': 'å¹³å‡é€šè¡Œé‡',
                'system_mean_travel_time': 'è¡Œç¨‹æ—¶é—´',
                'system_total_fuel_consumption': 'ç‡ƒæ²¹æ¶ˆè€—',
                'system_total_co2_emission': 'CO2æ’æ”¾'
            }.get(metric, metric)
            
            best_performers_text += f"ğŸ† {metric_name}: {performer['algorithm']}\n"
        
        ax.text(0.1, 0.9, best_performers_text, transform=ax.transAxes, fontsize=11,
               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))
        
        # 4. æ•°æ®ç»Ÿè®¡ä¿¡æ¯ (ä¸­æ’å³)
        ax = fig.add_subplot(gs[1, 2])
        ax.axis('off')
        
        stats_text = "æ•°æ®ç»Ÿè®¡ä¿¡æ¯:\n\n"
        for algorithm, stats in algorithm_stats.items():
            stats_text += f"{algorithm}:\n"
            stats_text += f"  æ–‡ä»¶æ•°: {stats['file_count']}\n"
            stats_text += f"  è®°å½•æ•°: {stats['total_records']}\n\n"
        
        ax.text(0.1, 0.9, stats_text, transform=ax.transAxes, fontsize=10,
               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
        
        # 5. æ”¹å–„ç‡å¯¹æ¯” (ä¸‹æ’)
        improvement_metrics = ['system_mean_waiting_time', 'system_mean_speed', 'system_total_throughput']
        
        for i, metric in enumerate(improvement_metrics):
            if i >= 3:  # æœ€å¤šæ˜¾ç¤º3ä¸ªæŒ‡æ ‡
                break
                
            ax = fig.add_subplot(gs[2, i])
            
            if metric in analysis_results['comparison_matrix']:
                comparisons = analysis_results['comparison_matrix'][metric]['comparisons']
                
                if comparisons:
                    comparison_names = []
                    improvement_rates = []
                    
                    for comp_name, comp_data in comparisons.items():
                        comparison_names.append(comp_name.replace('_vs_', ' vs '))
                        improvement_rates.append(comp_data['improvement_rate'])
                    
                    colors_list = ['green' if rate > 0 else 'red' for rate in improvement_rates]
                    bars = ax.barh(comparison_names, improvement_rates, color=colors_list, alpha=0.7)
                    
                    for i, (bar, rate) in enumerate(zip(bars, improvement_rates)):
                        # é¿å…ä½¿ç”¨+å·æ ¼å¼åŒ–ï¼Œé˜²æ­¢Ubuntuç³»ç»Ÿä¸­çš„ç¬¦å·ä¹±ç 
                        rate_text = f'{rate:.1f}%' if rate >= 0 else f'{rate:.1f}%'
                        if rate > 0:
                            rate_text = f'+{rate:.1f}%'
                        ax.text(rate + (2 if rate > 0 else -2), i, rate_text, 
                               va='center', ha='left' if rate > 0 else 'right')
                    
                    ax.axvline(x=0, color='black', linestyle='-', alpha=0.3)
                    
                    metric_name = {
                        'system_mean_waiting_time': 'ç­‰å¾…æ—¶é—´',
                        'system_mean_speed': 'å¹³å‡é€Ÿåº¦',
                        'system_total_throughput': 'å¹³å‡é€šè¡Œé‡'
                    }.get(metric, metric)
                    
                    ax.set_title(f'{metric_name}æ”¹å–„ç‡', fontweight='bold', fontsize=12)
                    ax.set_xlabel('æ”¹å–„ç‡ (%)')
        
        plt.suptitle(f'{analysis_results["intersection_name"]}è·¯å£ - å¤šç®—æ³•æ€§èƒ½å¯¹æ¯”åˆ†æ', 
                    fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"å¤šç®—æ³•å¯¹æ¯”åˆ†æå›¾è¡¨å·²ä¿å­˜: {output_path}")
    
    def _create_comparison_plots(self, ppo_data: pd.DataFrame, fixtime_data: pd.DataFrame, 
                               analysis_results: dict, output_path: str):
        """åˆ›å»ºå¯¹æ¯”åˆ†æå›¾è¡¨"""
        fig = plt.figure(figsize=(20, 16))
        
        # åˆ›å»ºç½‘æ ¼å¸ƒå±€
        gs = fig.add_gridspec(3, 3, height_ratios=[1, 1, 1], width_ratios=[1, 1, 1])
        
        # 1. PPOè®­ç»ƒè¿‡ç¨‹æ›²çº¿ (ä¸Šæ’)
        training_metrics = [
            ('system_mean_waiting_time', 'å¹³å‡ç­‰å¾…æ—¶é—´(ç§’)', 'red'),
            ('system_mean_speed', 'å¹³å‡é€Ÿåº¦(m/s)', 'blue'),
            ('system_total_throughput', 'æ€»é€šè¡Œé‡(è¾†)', 'green')
        ]
        
        for i, (metric, title, color) in enumerate(training_metrics):
            ax = fig.add_subplot(gs[0, i])
            if metric in ppo_data.columns:
                # åŸå§‹æ•°æ®
                ax.plot(ppo_data['step'], ppo_data[metric], alpha=0.3, color=color, linewidth=0.5)
                
                # ç§»åŠ¨å¹³å‡
                window = max(1, len(ppo_data) // 20)
                moving_avg = ppo_data[metric].rolling(window=window, center=True).mean()
                ax.plot(ppo_data['step'], moving_avg, color=color, linewidth=2, label='PPOè®­ç»ƒæ›²çº¿')
                
                # å›ºå®šå‘¨æœŸåŸºå‡†çº¿
                fixtime_mapping = {
                    'system_mean_waiting_time': 'avg_waiting_time',
                    'system_mean_speed': 'avg_speed',
                    'system_total_throughput': 'total_throughput'
                }
                
                if metric in fixtime_mapping and fixtime_mapping[metric] in fixtime_data.columns:
                    fixtime_val = fixtime_data[fixtime_mapping[metric]].iloc[0]
                    ax.axhline(y=fixtime_val, color='red', linestyle='--', alpha=0.7, label='å›ºå®šå‘¨æœŸåŸºå‡†')
                
                ax.set_title(f'PPOè®­ç»ƒè¿‡ç¨‹ - {title}', fontweight='bold', fontsize=12)
                ax.set_xlabel('è®­ç»ƒæ­¥æ•°')
                ax.set_ylabel(title)
                ax.grid(True, alpha=0.3)
                ax.legend()
        
        # 2. æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾ (ä¸­æ’)
        comparison_metrics = ['å¹³å‡ç­‰å¾…æ—¶é—´', 'å¹³å‡é€Ÿåº¦', 'æ€»é€šè¡Œé‡']
        for i, metric in enumerate(comparison_metrics):
            if metric in analysis_results['comparison_data']:
                ax = fig.add_subplot(gs[1, i])
                data = analysis_results['comparison_data'][metric]
                
                bars = ax.bar(['å›ºå®šå‘¨æœŸ', 'PPOç®—æ³•'], [data['fixtime'], data['ppo']], 
                             color=['orange', 'skyblue'], alpha=0.7)
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{height:.1f}', ha='center', va='bottom')
                
                # æ·»åŠ æ”¹å–„ç‡æ ‡æ³¨
                improvement = data['improvement']
                ax.text(0.5, max(data['fixtime'], data['ppo']) * 1.1, 
                       f'æ”¹å–„ç‡: {improvement:+.1f}%', 
                       ha='center', va='bottom', fontweight='bold',
                       color='green' if improvement > 0 else 'red')
                
                ax.set_title(f'{metric}å¯¹æ¯”', fontweight='bold', fontsize=12)
                ax.set_ylabel(metric)
        
        # 3. è®­ç»ƒé˜¶æ®µåˆ†æ (ä¸‹æ’å·¦)
        ax = fig.add_subplot(gs[2, 0])
        phases = list(analysis_results['phase_analysis'].keys())
        waiting_times = [analysis_results['phase_analysis'][p]['avg_waiting'] for p in phases]
        
        bars = ax.bar(phases, waiting_times, color=['lightcoral', 'gold', 'lightgreen'], alpha=0.7)
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.1f}', ha='center', va='bottom')
        
        ax.set_title('PPOè®­ç»ƒé˜¶æ®µ - ç­‰å¾…æ—¶é—´å˜åŒ–', fontweight='bold', fontsize=12)
        ax.set_ylabel('å¹³å‡ç­‰å¾…æ—¶é—´(ç§’)')
        
        # 4. æ”¹å–„ç‡é›·è¾¾å›¾ (ä¸‹æ’ä¸­)
        ax = fig.add_subplot(gs[2, 1])
        metrics = list(analysis_results['improvements'].keys())
        improvements = list(analysis_results['improvements'].values())
        
        colors = ['green' if imp > 0 else 'red' for imp in improvements]
        bars = ax.barh(metrics, improvements, color=colors, alpha=0.7)
        
        for i, (bar, imp) in enumerate(zip(bars, improvements)):
            ax.text(imp + (5 if imp > 0 else -5), i, f'{imp:+.1f}%', 
                   va='center', ha='left' if imp > 0 else 'right')
        
        ax.axvline(x=0, color='black', linestyle='-', alpha=0.3)
        ax.set_title('å„æŒ‡æ ‡æ”¹å–„ç‡å¯¹æ¯”', fontweight='bold', fontsize=12)
        ax.set_xlabel('æ”¹å–„ç‡ (%)')
        
        # 5. ç»¼åˆè¯„ä»· (ä¸‹æ’å³)
        ax = fig.add_subplot(gs[2, 2])
        ax.axis('off')
        
        # ç»Ÿè®¡ä¿¡æ¯
        positive_count = sum(1 for imp in improvements if imp > 0)
        total_count = len(improvements)
        avg_improvement = analysis_results['avg_improvement']
        
        summary_text = f"""
ç»¼åˆè¯„ä»·ç»“æœ

æ€»ä½“æ”¹å–„ç‡: {avg_improvement:+.1f}%

PPOä¼˜åŠ¿æŒ‡æ ‡: {positive_count}/{total_count}

è®­ç»ƒæ•°æ®é‡: {analysis_results['ppo_stats']['total_records']}æ¡
è®­ç»ƒæ€»æ­¥æ•°: {analysis_results['ppo_stats']['total_steps']:.0f}æ­¥

ç»“è®º: {'PPOç®—æ³•æ˜¾è‘—ä¼˜äºå›ºå®šå‘¨æœŸ' if avg_improvement > 20 else 'PPOç®—æ³•ç•¥ä¼˜äºå›ºå®šå‘¨æœŸ' if avg_improvement > 0 else 'å›ºå®šå‘¨æœŸç•¥ä¼˜äºPPOç®—æ³•'}
        """
        
        ax.text(0.1, 0.9, summary_text, transform=ax.transAxes, fontsize=11,
               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
        
        plt.suptitle('PPOç®—æ³• vs å›ºå®šå‘¨æœŸä¿¡å·æ§åˆ¶ - ç»¼åˆå¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"å¯¹æ¯”åˆ†æå›¾è¡¨å·²ä¿å­˜: {output_path}")
    
    def _generate_multi_algorithm_report(self, algorithm_stats: Dict, analysis_results: Dict, output_path: str):
        """ç”Ÿæˆå¤šç®—æ³•è¯¦ç»†åˆ†ææŠ¥å‘Š"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(f"# {analysis_results['intersection_name']}è·¯å£å¤šç®—æ³•æ€§èƒ½å¯¹æ¯”åˆ†ææŠ¥å‘Š\n\n")
            
            # 1. æ¦‚è§ˆ
            f.write("## 1. åˆ†ææ¦‚è§ˆ\n\n")
            f.write(f"- è·¯å£åç§°: {analysis_results['intersection_name']}\n")
            f.write(f"- åˆ†æç®—æ³•: {', '.join(analysis_results['algorithms'])}\n")
            f.write(f"- ç»¼åˆæœ€ä½³ç®—æ³•: {analysis_results['best_overall']}\n\n")
            
            # 2. æ•°æ®ç»Ÿè®¡
            f.write("## 2. æ•°æ®ç»Ÿè®¡\n\n")
            f.write("| ç®—æ³• | æ–‡ä»¶æ•° | è®°å½•æ•° | æ•°æ®æ¥æº |\n")
            f.write("|------|--------|--------|----------|\n")
            
            for algorithm, stats in algorithm_stats.items():
                source_files = stats['raw_data']['source_file'].unique()
                f.write(f"| {algorithm} | {stats['file_count']} | {stats['total_records']} | {', '.join(source_files[:3])}{'...' if len(source_files) > 3 else ''} |\n")
            
            f.write("\n")
            
            # 3. å…³é”®æŒ‡æ ‡å¯¹æ¯”
            f.write("## 3. å…³é”®æŒ‡æ ‡å¯¹æ¯”\n\n")
            
            metrics_info = {
                'system_mean_waiting_time': ('å¹³å‡ç­‰å¾…æ—¶é—´', 'ç§’', False),
                'system_mean_speed': ('å¹³å‡é€Ÿåº¦', 'm/s', True),
                'system_total_throughput': ('å¹³å‡é€šè¡Œé‡', 'è¾†/å°æ—¶', True),
                'system_mean_travel_time': ('å¹³å‡è¡Œç¨‹æ—¶é—´', 'ç§’', False),
                'system_total_fuel_consumption': ('æ€»ç‡ƒæ²¹æ¶ˆè€—', 'L', False),
                'system_total_co2_emission': ('æ€»CO2æ’æ”¾', 'g', False)
            }
            
            for metric, (name, unit, is_higher_better) in metrics_info.items():
                if metric in analysis_results['comparison_matrix']:
                    f.write(f"### {name}\n\n")
                    
                    values = analysis_results['comparison_matrix'][metric]['values']
                    if values:
                        f.write("| ç®—æ³• | æ•°å€¼ | æ’å |\n")
                        f.write("|------|------|------|\n")
                        
                        # æ’åº
                        sorted_values = sorted(values.items(), 
                                             key=lambda x: x[1], 
                                             reverse=is_higher_better)
                        
                        for rank, (algorithm, value) in enumerate(sorted_values, 1):
                            rank_emoji = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else f"{rank}"
                            f.write(f"| {algorithm} | {value:.2f} {unit} | {rank_emoji} |\n")
                        
                        f.write("\n")
                        
                        # æ”¹å–„ç‡åˆ†æ
                        comparisons = analysis_results['comparison_matrix'][metric]['comparisons']
                        if comparisons:
                            f.write("**æ”¹å–„ç‡åˆ†æ:**\n\n")
                            for comp_name, comp_data in comparisons.items():
                                best_alg, compared_alg = comp_name.split('_vs_')
                                improvement = comp_data['improvement_rate']
                                f.write(f"- {best_alg} ç›¸æ¯” {compared_alg}: {improvement:+.1f}%\n")
                            f.write("\n")
            
            # 4. ç»¼åˆè¯„åˆ†
            f.write("## 4. ç»¼åˆè¯„åˆ†\n\n")
            f.write("åŸºäºåŠ æƒè¯„åˆ†ç³»ç»Ÿçš„ç»¼åˆæ€§èƒ½æ’å:\n\n")
            f.write("| æ’å | ç®—æ³• | ç»¼åˆè¯„åˆ† | ä¼˜åŠ¿æŒ‡æ ‡ |\n")
            f.write("|------|------|----------|----------|\n")
            
            sorted_scores = sorted(analysis_results['overall_scores'].items(), 
                                 key=lambda x: x[1], reverse=True)
            
            for rank, (algorithm, score) in enumerate(sorted_scores, 1):
                # æ‰¾å‡ºè¯¥ç®—æ³•çš„ä¼˜åŠ¿æŒ‡æ ‡
                advantages = []
                for metric, performer in analysis_results['best_performers'].items():
                    if performer['algorithm'] == algorithm:
                        metric_name = metrics_info.get(metric, (metric, '', True))[0]
                        advantages.append(metric_name)
                
                rank_emoji = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else f"{rank}"
                f.write(f"| {rank_emoji} | {algorithm} | {score:.3f} | {', '.join(advantages) if advantages else 'æ— '} |\n")
            
            f.write("\n")
            
            # 5. è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
            f.write("## 5. è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯\n\n")
            
            for algorithm, stats in algorithm_stats.items():
                f.write(f"### {algorithm}ç®—æ³•\n\n")
                f.write(f"- æ•°æ®æ–‡ä»¶: {stats['file_count']}ä¸ª\n")
                f.write(f"- æ€»è®°å½•æ•°: {stats['total_records']}æ¡\n\n")
                
                f.write("**æŒ‡æ ‡ç»Ÿè®¡:**\n\n")
                f.write("| æŒ‡æ ‡ | å‡å€¼ | æ ‡å‡†å·® | æœ€å°å€¼ | æœ€å¤§å€¼ | ä¸­ä½æ•° |\n")
                f.write("|------|------|--------|--------|--------|--------|\n")
                
                for metric, metric_stats in stats['metrics'].items():
                    metric_name = metrics_info.get(metric, (metric, '', True))[0]
                    f.write(f"| {metric_name} | {metric_stats['mean']:.2f} | {metric_stats['std']:.2f} | {metric_stats['min']:.2f} | {metric_stats['max']:.2f} | {metric_stats['median']:.2f} |\n")
                
                f.write("\n")
            
            # 6. ç»“è®ºä¸å»ºè®®
            f.write("## 6. ç»“è®ºä¸å»ºè®®\n\n")
            
            best_algorithm = analysis_results['best_overall']
            best_score = analysis_results['overall_scores'][best_algorithm]
            
            if best_score > 0.6:
                conclusion = f"{best_algorithm}ç®—æ³•åœ¨ç»¼åˆè¯„ä¼°ä¸­è¡¨ç°å“è¶Šï¼Œå»ºè®®ä¼˜å…ˆé‡‡ç”¨ã€‚"
            elif best_score > 0.4:
                conclusion = f"{best_algorithm}ç®—æ³•åœ¨ç»¼åˆè¯„ä¼°ä¸­è¡¨ç°è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘é‡‡ç”¨ã€‚"
            else:
                conclusion = "å„ç®—æ³•è¡¨ç°ç›¸è¿‘ï¼Œå»ºè®®æ ¹æ®å…·ä½“åœºæ™¯éœ€æ±‚é€‰æ‹©ã€‚"
            
            f.write(f"**ä¸»è¦ç»“è®º:** {conclusion}\n\n")
            
            # å…·ä½“å»ºè®®
            f.write("**å…·ä½“å»ºè®®:**\n\n")
            for metric, performer in analysis_results['best_performers'].items():
                metric_name = metrics_info.get(metric, (metric, '', True))[0]
                f.write(f"- è‹¥ä¼˜å…ˆè€ƒè™‘{metric_name}ï¼Œæ¨èä½¿ç”¨{performer['algorithm']}ç®—æ³•\n")
            
            f.write("\n")
            f.write("**æ³¨æ„äº‹é¡¹:**\n\n")
            f.write("- æœ¬åˆ†æåŸºäºå†å²è®­ç»ƒæ•°æ®ï¼Œå®é™…éƒ¨ç½²æ•ˆæœå¯èƒ½å› ç¯å¢ƒå˜åŒ–è€Œæœ‰æ‰€å·®å¼‚\n")
            f.write("- å»ºè®®åœ¨å®é™…åº”ç”¨å‰è¿›è¡Œå°è§„æ¨¡è¯•ç‚¹éªŒè¯\n")
            f.write("- å®šæœŸç›‘æ§å’Œè¯„ä¼°ç®—æ³•æ€§èƒ½ï¼Œå¿…è¦æ—¶è¿›è¡Œè°ƒä¼˜\n")
        
        self.logger.info(f"å¤šç®—æ³•åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    def analyze_training_comparison_legacy(self, ppo_file: str, fixtime_file: str) -> Tuple[Optional[str], str]:
        """
        æ—§ç‰ˆæœ¬çš„è®­ç»ƒå¯¹æ¯”åˆ†ææ–¹æ³•ï¼ˆä¿æŒå‘åå…¼å®¹æ€§ï¼‰
        
        Args:
            ppo_file: PPOè®­ç»ƒæ—¥å¿—æ–‡ä»¶è·¯å¾„
            fixtime_file: å›ºå®šå‘¨æœŸç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            Tuple[Optional[str], str]: (åˆ†æç»“æœå›¾ç‰‡è·¯å¾„, è¾“å‡ºä¿¡æ¯)
        """
        try:
            # éªŒè¯è¾“å…¥æ–‡ä»¶
            if not ppo_file or not fixtime_file:
                return None, "âŒ è¯·æä¾›PPOå’Œå›ºå®šå‘¨æœŸæ–‡ä»¶è·¯å¾„"
                
            if not self.file_manager.file_exists(ppo_file):
                return None, f"âŒ PPOæ–‡ä»¶ä¸å­˜åœ¨: {ppo_file}"
                
            if not self.file_manager.file_exists(fixtime_file):
                return None, f"âŒ å›ºå®šå‘¨æœŸæ–‡ä»¶ä¸å­˜åœ¨: {fixtime_file}"
            
            # åŠ è½½æ•°æ®
            self.logger.info("å¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®...")
            ppo_data = pd.read_csv(ppo_file)
            fixtime_data = pd.read_csv(fixtime_file)
            
            # æ‰§è¡Œå¯¹æ¯”åˆ†æ
            analysis_results = self._perform_comparative_analysis(ppo_data, fixtime_data)
            
            # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
            output_dir = os.path.dirname(ppo_file)
            plot_path = os.path.join(output_dir, "comparative_analysis.png")
            self._create_comparison_plots(ppo_data, fixtime_data, analysis_results, plot_path)
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report_path = os.path.join(output_dir, "comparative_analysis_report.md")
            self._generate_analysis_report(analysis_results, report_path)
            
            # è¾“å‡ºç»“æœæ‘˜è¦
            success_msg = f"âœ… å¯¹æ¯”åˆ†æå®Œæˆ\n" \
                         f"ğŸ“Š PPOè®­ç»ƒè®°å½•: {analysis_results['ppo_stats']['total_records']}æ¡\n" \
                         f"ğŸ“Š å›ºå®šå‘¨æœŸè®°å½•: {analysis_results['fixtime_stats']['total_records']}æ¡\n" \
                         f"ğŸ“ˆ å¹³å‡æ”¹å–„ç‡: {analysis_results['avg_improvement']:+.1f}%\n" \
                         f"ğŸ–¼ï¸ å¯è§†åŒ–å›¾è¡¨: {plot_path}\n" \
                         f"ğŸ“„ åˆ†ææŠ¥å‘Š: {report_path}"
            
            self.logger.info("å¯¹æ¯”åˆ†ææˆåŠŸå®Œæˆ")
            return plot_path, success_msg
            
        except ValidationError as e:
            error_msg = f"âŒ è¾“å…¥éªŒè¯å¤±è´¥: {e}"
            self.logger.warning(error_msg)
            return None, error_msg
            
        except FileOperationError as e:
            error_msg = f"âŒ æ–‡ä»¶æ“ä½œå¤±è´¥: {e}"
            self.logger.error(error_msg)
            return None, error_msg
            
        except Exception as e:
            error_msg = f"âŒ å¯¹æ¯”åˆ†ææ—¶å‘ç”Ÿé”™è¯¯: {e}"
            self.logger.error(f"{error_msg}\n{traceback.format_exc()}")
            return None, error_msg
    
    def _generate_analysis_report(self, analysis_results: dict, output_path: str):
        """ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# PPOç®—æ³•ä¸å›ºå®šå‘¨æœŸä¿¡å·æ§åˆ¶å¯¹æ¯”åˆ†ææŠ¥å‘Š\n\n")
            
            # æ•°æ®æ¦‚è§ˆ
            f.write("## 1. æ•°æ®æ¦‚è§ˆ\n\n")
            f.write(f"- PPOè®­ç»ƒæ•°æ®: {analysis_results['ppo_stats']['total_records']}æ¡è®°å½•\n")
            f.write(f"- è®­ç»ƒæ€»æ­¥æ•°: {analysis_results['ppo_stats']['total_steps']:.0f}æ­¥\n")
            f.write(f"- å›ºå®šå‘¨æœŸæ•°æ®: {analysis_results['fixtime_stats']['total_records']}æ¡è®°å½•\n\n")
            
            # æ€§èƒ½å¯¹æ¯”
            f.write("## 2. æ€§èƒ½å¯¹æ¯”ç»“æœ\n\n")
            f.write("| æŒ‡æ ‡ | å›ºå®šå‘¨æœŸ | PPOç®—æ³• | æ”¹å–„ç‡ |\n")
            f.write("|------|----------|---------|--------|\n")
            
            for metric, data in analysis_results['comparison_data'].items():
                f.write(f"| {metric} | {data['fixtime']:.2f} | {data['ppo']:.2f} | {data['improvement']:+.1f}% |\n")
            
            f.write(f"\n**å¹³å‡æ”¹å–„ç‡**: {analysis_results['avg_improvement']:+.1f}%\n\n")
            
            # è®­ç»ƒè¿‡ç¨‹åˆ†æ
            f.write("## 3. PPOè®­ç»ƒè¿‡ç¨‹åˆ†æ\n\n")
            for phase, data in analysis_results['phase_analysis'].items():
                f.write(f"### {phase}é˜¶æ®µ\n")
                f.write(f"- å¹³å‡ç­‰å¾…æ—¶é—´: {data['avg_waiting']:.2f}ç§’\n")
                f.write(f"- å¹³å‡é€Ÿåº¦: {data['avg_speed']:.2f}m/s\n")
                f.write(f"- å¹³å‡é€šè¡Œé‡: {data['avg_throughput']:.2f}è¾†\n\n")
            
            # ç»“è®º
            avg_imp = analysis_results['avg_improvement']
            if avg_imp > 20:
                conclusion = "PPOç®—æ³•åœ¨å¤šæ•°æŒ‡æ ‡ä¸Šæ˜¾è‘—ä¼˜äºå›ºå®šå‘¨æœŸæ§åˆ¶ï¼Œå»ºè®®ä¼˜å…ˆé‡‡ç”¨ã€‚"
            elif avg_imp > 0:
                conclusion = "PPOç®—æ³•æ•´ä½“ä¼˜äºå›ºå®šå‘¨æœŸæ§åˆ¶ï¼Œä½†ä¼˜åŠ¿ä¸å¤Ÿæ˜æ˜¾ï¼Œéœ€è¦ç»“åˆå…·ä½“åœºæ™¯é€‰æ‹©ã€‚"
            else:
                conclusion = "å›ºå®šå‘¨æœŸæ§åˆ¶åœ¨å½“å‰åœºæ™¯ä¸‹è¡¨ç°æ›´å¥½ï¼Œå»ºè®®ä¿æŒç°æœ‰æ–¹æ¡ˆã€‚"
            
            f.write(f"## 4. ç»“è®ºä¸å»ºè®®\n\n{conclusion}\n")
        
        self.logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {output_path}")

# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°
def analyze_training_files(sample_file_path: str) -> Tuple[Optional[str], str]:
    """
    ä¾¿æ·å‡½æ•°ï¼šåŸºäºæ ·æœ¬æ–‡ä»¶åˆ†æå¤šç®—æ³•è®­ç»ƒå¯¹æ¯”
    
    Args:
        sample_file_path: æ ·æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚zfdx-PPO_conn0_ep1.csvï¼‰
        
    Returns:
        Tuple[Optional[str], str]: (å›¾ç‰‡è·¯å¾„, ç»“æœä¿¡æ¯)
    """
    analyzer = ComparativeAnalyzer()
    return analyzer.analyze_training_comparison(sample_file_path)

def analyze_training_files_legacy(ppo_file: str, fixtime_file: str) -> Tuple[Optional[str], str]:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ†æè®­ç»ƒæ–‡ä»¶å¯¹æ¯”ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
    
    Args:
        ppo_file: PPOè®­ç»ƒæ—¥å¿—æ–‡ä»¶è·¯å¾„
        fixtime_file: å›ºå®šå‘¨æœŸç»“æœæ–‡ä»¶è·¯å¾„
        
    Returns:
        Tuple[Optional[str], str]: (å›¾ç‰‡è·¯å¾„, ç»“æœä¿¡æ¯)
    """
    analyzer = ComparativeAnalyzer()
    return analyzer.analyze_training_comparison_legacy(ppo_file, fixtime_file)

if __name__ == "__main__":
    import sys
    import argparse
    
    # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='å¤šç®—æ³•å¯¹æ¯”åˆ†æå·¥å…·')
    parser.add_argument('directory', help='åŒ…å«å¤šç§ç®—æ³•ç»“æœæ–‡ä»¶çš„ç›®å½•è·¯å¾„')
    parser.add_argument('--sample-file', help='æ ·æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºæŒ‡å®šç‰¹å®šçš„æ ·æœ¬æ–‡ä»¶ï¼‰')
    
    # å¦‚æœæ²¡æœ‰æä¾›å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•
    if len(sys.argv) == 1:
        # æµ‹è¯•ç”¨ä¾‹ - æ–°çš„å¤šç®—æ³•åˆ†æ
        sample_file = "/Users/xnpeng/sumoptis/atscui/outs/train/zfdx-PPO_conn0_ep1.csv"
        
        result_path, message = analyze_training_files(sample_file)
        print(message)
        if result_path:
            print(f"å¤šç®—æ³•åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {result_path}")
    else:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = parser.parse_args()
        
        try:
            if args.sample_file:
                # ä½¿ç”¨æŒ‡å®šçš„æ ·æœ¬æ–‡ä»¶
                result_path, message = analyze_training_files(args.sample_file)
            else:
                # åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾æ ·æœ¬æ–‡ä»¶
                import glob
                pattern = os.path.join(args.directory, "*-PPO_conn0_ep*.csv")
                sample_files = glob.glob(pattern)
                
                if not sample_files:
                    # å°è¯•å…¶ä»–ç®—æ³•æ¨¡å¼
                    patterns = [
                        "*-DQN_conn0_ep*.csv",
                        "*-A2C_conn0_ep*.csv",
                        "*-SAC_conn0_ep*.csv"
                    ]
                    for pattern_template in patterns:
                        pattern = os.path.join(args.directory, pattern_template)
                        sample_files = glob.glob(pattern)
                        if sample_files:
                            break
                
                if sample_files:
                    # ä½¿ç”¨æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªæ ·æœ¬æ–‡ä»¶
                    sample_file = sample_files[0]
                    result_path, message = analyze_training_files(sample_file)
                else:
                    message = f"âŒ åœ¨ç›®å½• {args.directory} ä¸­æœªæ‰¾åˆ°åˆé€‚çš„ç®—æ³•ç»“æœæ–‡ä»¶"
                    result_path = None
            
            print(message)
            if result_path:
                print(f"å¤šç®—æ³•åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {result_path}")
                
        except Exception as e:
            print(f"âŒ æ‰§è¡Œåˆ†ææ—¶å‘ç”Ÿé”™è¯¯: {e}")
            sys.exit(1)